The is extracted from internet for 8,00,000 + rows thus subsets are extracted from this data set,
Taking Logistic Regression as base model: 
  First subset consist of imbalanced data, and traditional approach direct encoding is followed for feature engineering
  Second subset consist of blalanced data and binning/bucketing is done for features which is cross checked with hypothesis testing then one-hot encoding is performed.
After doing data cleaning and feature engineering different ML models are built with the focus of achieving the best model as per classification report
Further, after model building clustering is done to identify critical and high risk cases, which needs focus from beginning to avoid high revenue losses.
